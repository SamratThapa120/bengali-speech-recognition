{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as pltm\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "longformdata = pd.read_csv(\"/app/dataset/metadata/annoated.csv\",delimiter=\"\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference mode is on\n"
     ]
    }
   ],
   "source": [
    "from configs.wav2vec2_characterwise_pretrained_ctc import Configs\n",
    "CFG = Configs(longformdata.file.apply(lambda x: os.path.join(\"/app/dataset/examples\",x)),longformdata.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model checkpoint from epoch:  20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.load_state_dict(\"/app/bengali-speech-recognition/workdir/wav2vec2_characterlevel_pretrained_ctcloss_noaugs/bestmodel_wer.pkl\")\n",
    "CFG.model.cuda()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(tensor, window_size, overlap):\n",
    "    \"\"\"\n",
    "    Slices the input tensor with a sliding window of given size and overlap.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor of shape (points,).\n",
    "        window_size (int): The size of the sliding window.\n",
    "        overlap (int): The overlap between consecutive windows.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the sliced windows stacked along the 0 dimension.\n",
    "    \"\"\"\n",
    "    step = window_size - overlap\n",
    "    slices = []\n",
    "    \n",
    "    for start in range(0, len(tensor) - window_size + 1, step):\n",
    "        end = start + window_size\n",
    "        slices.append(tensor[start:end])\n",
    "    \n",
    "    # Handle the final feature and pad it with zeros to match window_size\n",
    "    if len(tensor) % step != 0:\n",
    "        last_slice = tensor[-window_size:]\n",
    "        slices.append(torch.nn.functional.pad(last_slice, (0, window_size - len(last_slice))))\n",
    "    \n",
    "    return torch.stack(slices, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:06<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "all_truth= []\n",
    "all_preds = []\n",
    "OVERLAP=1\n",
    "for feature,truth in tqdm(CFG.inference_dataset):\n",
    "    with torch.no_grad():\n",
    "        feature = sliding_window(feature,CFG.MAX_AUDIO_LENGTH,OVERLAP).cuda()\n",
    "        preds = CFG.model(feature)\n",
    "        all_indices = torch.argmax(preds.detach().cpu(), dim=-1)\n",
    "    generated = []\n",
    "    for indices in all_indices:\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = indices[indices != CFG.BLANK_TOKEN]\n",
    "        generated.extend(CFG.tokenizer.decode_torch_inference(indices))\n",
    "    prediction = \"\".join(generated)\n",
    "    all_truth.append(CFG.tokenizer.decode_torch_inference(truth))\n",
    "    all_preds.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "longformdata[\"predictions\"] = all_preds\n",
    "longformdata[\"truth\"] = all_truth\n",
    "longformdata[\"wer\"] = [wer(a,b) for a,b in zip(all_truth,all_preds)]\n",
    "longformdata[\"cer\"] = [cer(a,b) for a,b in zip(all_truth,all_preds)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8809398167745288, 0.549489238711458)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformdata.wer.mean(),longformdata.cer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8483031106888939, 0.4413356898550691)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformdata.wer.mean(),longformdata.cer.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
