{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as pltm\n",
    "sys.path.append(\"../\")\n",
    "from bengali_asr.dataset.encoder_decoder_dataset import load_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "longformdata = pd.read_csv(\"/app/dataset/metadata/annoated.csv\",delimiter=\"\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference mode is on\n"
     ]
    }
   ],
   "source": [
    "from configs.whisper_characterwise_pretrained_augs_small_openasr import Configs\n",
    "CFG = Configs(longformdata.file.apply(lambda x: os.path.join(\"/app/dataset/examples\",x)).tolist(),longformdata.sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CFG.inference_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 2668])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(data[0][:,:600].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model checkpoint from epoch:  135000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.load_state_dict(\"/app/bengali-speech-recognition/workdir/whispersmall_characterlevel_finetuned_augmentations_openasr/bestmodel_wer.pkl\")\n",
    "CFG.model.cuda()\n",
    "CFG.model.eval()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(tensor, window_size, overlap):\n",
    "    \"\"\"\n",
    "    Slices the input tensor with a sliding window of given size and overlap.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor of shape (points,).\n",
    "        window_size (int): The size of the sliding window.\n",
    "        overlap (int): The overlap between consecutive windows.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the sliced windows stacked along the 0 dimension.\n",
    "    \"\"\"\n",
    "    step = window_size - overlap\n",
    "    slices = []\n",
    "    \n",
    "    for start in range(0, len(tensor) - window_size + 1, step):\n",
    "        end = start + window_size\n",
    "        slices.append(tensor[start:end])\n",
    "    \n",
    "    # Handle the final feature and pad it with zeros to match window_size\n",
    "    if len(tensor) % step != 0:\n",
    "        last_slice = tensor[-window_size:]\n",
    "        slices.append(torch.nn.functional.pad(last_slice, (0, window_size - len(last_slice))))\n",
    "    \n",
    "    return torch.stack(slices, dim=0)\n",
    "def sliding_window_spectro(tensor, window_size, overlap):\n",
    "    \"\"\"\n",
    "    Slices the input tensor with a sliding window of given size and overlap.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor of shape (mels,points).\n",
    "        window_size (int): The size of the sliding window.\n",
    "        overlap (int): The overlap between consecutive windows.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the sliced windows stacked along the 0 dimension.\n",
    "    \"\"\"\n",
    "    step = window_size - overlap\n",
    "    slices = []\n",
    "    \n",
    "    for start in range(0, tensor.shape[1] - window_size + 1, step):\n",
    "        end = start + window_size\n",
    "        slices.append(tensor[:,start:end])\n",
    "    \n",
    "    # Handle the final feature and pad it with zeros to match window_size\n",
    "    if tensor.shape[1] % step != 0:\n",
    "        last_slice = tensor[:,-window_size:]\n",
    "        slices.append(torch.nn.functional.pad(last_slice, (0, window_size - last_slice.shape[1])))\n",
    "    \n",
    "    return torch.stack(slices, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ctc inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_truth= []\n",
    "# all_preds = []\n",
    "# OVERLAP=1\n",
    "# for feature,truth in tqdm(CFG.inference_dataset):\n",
    "#     with torch.no_grad():\n",
    "#         feature = sliding_window(feature,CFG.MAX_AUDIO_LENGTH,OVERLAP).cuda()\n",
    "#         preds = CFG.model(feature)\n",
    "#         all_indices = torch.argmax(preds.detach().cpu(), dim=-1)\n",
    "#     generated = []\n",
    "#     for indices in all_indices:\n",
    "#         indices = torch.unique_consecutive(indices, dim=-1)\n",
    "#         indices = indices[indices != CFG.BLANK_TOKEN]\n",
    "#         generated.extend(CFG.tokenizer.decode_torch_inference(indices))\n",
    "#     prediction = \"\".join(generated)\n",
    "#     all_truth.append(CFG.tokenizer.decode_torch_inference(truth))\n",
    "#     all_preds.append(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoreg inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CFG.inference_dataset.speech_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:20,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "all_truth= []\n",
    "all_preds = []\n",
    "OVERLAP=1\n",
    "for feature,truth in tqdm(iter(CFG.inference_dataset)):\n",
    "    with torch.no_grad():\n",
    "        inputs = sliding_window_spectro(feature,3000,OVERLAP).cuda()\n",
    "        batch_size = inputs.size(0)\n",
    "        generated_tokens = torch.ones((batch_size, 1), dtype=torch.long, device=\"cuda\") * CFG.START_TOKEN\n",
    "        encoded_logits = CFG.model.encoder(inputs)\n",
    "        eos_flags = torch.zeros(batch_size, dtype=torch.bool, device=\"cuda\")\n",
    "\n",
    "        for _ in range(CFG.MAX_PREDICTION_LENGTH):\n",
    "            logits = CFG.model.decoder(generated_tokens, encoded_logits)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "\n",
    "            # Update end-of-sequence flags\n",
    "            eos_flags = eos_flags | (next_token.squeeze(-1) == CFG.END_TOKEN)\n",
    "\n",
    "            # Stop condition: if all sequences in the batch have generated <eos>\n",
    "            if eos_flags.all():\n",
    "                break\n",
    "    generated_tokens = generated_tokens[:, 1:]  # Remove the start token\n",
    "    generated = []\n",
    "    for gen in generated_tokens:\n",
    "        end_pos = (gen == CFG.END_TOKEN).nonzero(as_tuple=True)[0]\n",
    "        if len(end_pos) > 0:\n",
    "            gen = gen[:end_pos[0]] \n",
    "        hypothesis = CFG.tokenizer.decode_torch_inference(gen)\n",
    "        generated.append(hypothesis)\n",
    "    prediction = \"\".join(generated)\n",
    "    all_truth.append(CFG.tokenizer.decode_torch_inference(truth))\n",
    "    all_preds.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "longformdata[\"predictions\"] = all_preds\n",
    "longformdata[\"truth\"] = all_truth\n",
    "longformdata[\"wer\"] = [wer(a,b) for a,b in zip(all_truth,all_preds)]\n",
    "longformdata[\"cer\"] = [cer(a,b) for a,b in zip(all_truth,all_preds)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     477\n",
       "1     710\n",
       "2     623\n",
       "3     445\n",
       "4     722\n",
       "5     267\n",
       "6     281\n",
       "7     499\n",
       "8     705\n",
       "9     531\n",
       "10    324\n",
       "11    233\n",
       "12    983\n",
       "13    447\n",
       "14    527\n",
       "15    432\n",
       "16    429\n",
       "Name: sentence, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformdata.sentence.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9793515432492308, 0.8634875577525151)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformdata.wer.mean(),longformdata.cer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7978960718556445, 0.5293556438877308)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformdata.wer.mean(),longformdata.cer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
